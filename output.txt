QUESTION 1 - My covariance matrix subtracted from numpy's
[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]


Yes, this covariance matrix is the matrix along the diagnol
Q2 - Print largest and smallest and dimension
Minimum Value:  [  4.57080123e-05  -5.73158422e-05   2.26331399e-02   5.51887659e-01
  -8.33611268e-01  -2.02449029e-06   6.63190171e-06  -4.52404054e-07
  -1.81578398e-05  -2.80193886e-06]
Dimension:  9
Max Value:  [ -3.27812305e-01  -1.13810001e-01  -3.06076690e-03   9.27109146e-04
   5.21882399e-04   3.80700011e-01   7.16522956e-02  -3.37405774e-03
   5.35411190e-02  -8.52426707e-01]
Dimension:  0


Question 3 - Use linalg.eig
Variance of datapoints in subspace:
[[ 6579.445     0.   ]
 [    0.     3853.668]]
Largest eigen values:
10433.113


Q4 = Covariance matrix E
[[-0.328 -0.134  0.066  0.864  0.095 -0.02   0.338  0.004  0.     0.   ]
 [-0.114 -0.054 -0.019  0.32  -0.02   0.023 -0.938  0.013  0.001  0.   ]
 [-0.003  0.001  0.001  0.007  0.003  0.    -0.011 -0.928 -0.372  0.023]
 [ 0.001  0.    -0.001 -0.003 -0.002  0.     0.003  0.322 -0.769  0.552]
 [ 0.001  0.     0.    -0.002 -0.001  0.     0.002  0.188 -0.519 -0.834]
 [ 0.381  0.694 -0.54   0.285 -0.005 -0.009  0.022  0.001  0.     0.   ]
 [ 0.072  0.563  0.813  0.076 -0.102  0.01  -0.029  0.002  0.     0.   ]
 [-0.003 -0.001 -0.012  0.012 -0.01   0.999  0.03   0.     0.     0.   ]
 [ 0.054 -0.118 -0.053  0.093 -0.984 -0.013  0.053 -0.003  0.001  0.   ]
 [-0.852  0.409 -0.199 -0.235 -0.107 -0.003  0.006  0.001  0.     0.   ]]


Q5 - see pca(data) method


Q6 - using pc, preserving 90%
[[  6.67710000e+01  -5.55980000e+01   1.37830000e+01  -2.39110000e+01
    4.80940000e+01  -9.21400000e+00  -2.96600000e+00  -1.98800000e+00
   -1.34900000e+00   1.10000000e-01]
 [  1.74588000e+02  -1.09892000e+02   3.39870000e+01   7.50000000e+00
    2.78530000e+01  -1.10700000e+01   7.91000000e-01  -1.64600000e+00
   -1.49300000e+00   3.50000000e-02]
 [  2.43441000e+02  -1.11280000e+02   1.62323000e+02  -1.58011000e+02
    8.40340000e+01  -4.87690000e+01  -6.41640000e+01  -1.42300000e+00
   -1.42000000e+00   9.40000000e-02]
 [  9.79340000e+01  -5.80040000e+01   4.22440000e+01  -4.31100000e+00
    2.00750000e+01  -8.22700000e+00   8.64000000e-01  -1.63100000e+00
   -1.51500000e+00   6.50000000e-02]
 [  3.31900000e+02  -1.45558000e+02   4.08440000e+01   7.84000000e+00
    3.91590000e+01   2.01320000e+01  -1.60400000e+00  -1.82700000e+00
   -1.43300000e+00   9.40000000e-02]
 [  1.99676000e+02  -1.48384000e+02   3.70250000e+01  -1.35480000e+01
    2.92460000e+01   8.41100000e+00  -6.79000000e-01  -1.86700000e+00
   -1.28900000e+00   8.80000000e-02]
 [  1.98342000e+02  -1.08914000e+02   1.47610000e+01  -1.56500000e+00
    2.78800000e+01   9.53300000e+00   9.30000000e-01  -2.08600000e+00
   -1.35500000e+00   8.30000000e-02]
 [  1.99694000e+02  -1.24541000e+02   4.50240000e+01   1.82400000e+01
    2.60250000e+01  -4.00200000e+00  -2.47600000e+00  -1.70000000e+00
   -1.33200000e+00   1.10000000e-01]
 [  2.00227000e+02  -2.09803000e+02   3.51640000e+01  -7.84730000e+01
    3.27560000e+01   4.13040000e+01  -8.20500000e+00  -2.47500000e+00
   -1.58500000e+00   1.06000000e-01]
 [  9.12160000e+01  -7.56200000e+01  -4.42300000e+00  -3.22360000e+01
    1.90750000e+01  -7.47800000e+00   1.60900000e+00  -1.72800000e+00
   -1.30300000e+00   8.60000000e-02]]


Q7 - Whitening Transformation
[[  0.89158643  -1.2580398    0.32204294  -0.47224275   2.65224337
   -0.41249389  -0.15564002  -7.14746693 -14.37610519   5.09046202]
 [  2.33126294  -2.48655561   0.79412113   0.14812476   1.53598735
   -0.49559236   0.04150149  -5.91650389 -15.9150918    1.6292975 ]
 [  3.25064145  -2.5179808    3.79273622  -3.1207441    4.63421969
   -2.1832736   -3.36724726  -5.11393922 -15.1397138    4.3256253 ]
 [  1.30769932  -1.3124694    0.98704575  -0.08515163   1.10705388
   -0.36830755   0.0453558   -5.86427582 -16.15347674   3.00428526]
 [  4.43183428  -3.2935945    0.95433985   0.15484605   2.15946747
    0.90126147  -0.08417512  -6.56724655 -15.27577468   4.35414023]
 [  2.66626236  -3.3575408    0.86509535  -0.26758276   1.61281367
    0.3765447   -0.03565149  -6.710677   -13.74161733   4.07009376]
 [  2.64843923  -2.46443839   0.3448939   -0.03091212   1.53746769
    0.426792     0.04880159  -7.49973493 -14.44284559   3.81171865]
 [  2.66649683  -2.8180318    1.05200992   0.36024378   1.43521259
   -0.17916226  -0.12992658  -6.1110856  -14.19605406   5.07860143]
 [  2.67361692  -4.74729528   0.8216286   -1.54984616   1.806411
    1.84909384  -0.43060919  -8.89605542 -16.89124253   4.87669612]
 [  1.21799584  -1.71107564  -0.10333381  -0.6366639    1.05189998
   -0.33478292   0.08445884  -6.21289225 -13.89082984   3.98110966]]


Q8 - Special Matrix Class?
YES - THIS IS THE IDENTITY MATRIX
[[ 1.         -0.66425617  0.44057648 -0.11520849  0.32913767  0.20964184
  -0.3068432   0.00976828 -0.14380185  0.11218842]
 [-0.66425617  1.         -0.12842554  0.1810215  -0.02934058 -0.63432508
   0.07112563  0.55191192  0.34221763 -0.25203634]
 [ 0.44057648 -0.12842554  1.         -0.74709115  0.84128566 -0.6059275
  -0.94213829  0.49811382 -0.19769959  0.06931181]
 [-0.11520849  0.1810215  -0.74709115  1.         -0.793562    0.40436326
   0.89552058 -0.08117127  0.22094766 -0.25916705]
 [ 0.32913767 -0.02934058  0.84128566 -0.793562    1.         -0.56198926
  -0.9162968   0.28471266 -0.04360084  0.29239759]
 [ 0.20964184 -0.63432508 -0.6059275   0.40436326 -0.56198926  1.
   0.63415957 -0.84890023 -0.26242997  0.2089599 ]
 [-0.3068432   0.07112563 -0.94213829  0.89552058 -0.9162968   0.63415957
   1.         -0.39577201  0.1083143  -0.18820372]
 [ 0.00976828  0.55191192  0.49811382 -0.08117127  0.28471266 -0.84890023
  -0.39577201  1.          0.24918508 -0.37848117]
 [-0.14380185  0.34221763 -0.19769959  0.22094766 -0.04360084 -0.26242997
   0.1083143   0.24918508  1.          0.30270683]
 [ 0.11218842 -0.25203634  0.06931181 -0.25916705  0.29239759  0.2089599
  -0.18820372 -0.37848117  0.30270683  1.        ]]
